{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cb087b",
   "metadata": {},
   "source": [
    "# Datasets and Learning Paradigms\n",
    "\n",
    "## What is a Dataset?\n",
    "\n",
    "A dataset is a collection of examples (data points or samples) containing input-output mappings that the model aims to learn.\n",
    "\n",
    "![Dataset Overview](images/dataset-overview.png)\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- Each example consists of:\n",
    "  - **Input** `x`: a feature vector (measurements/attributes)\n",
    "  - **Output** `Å·`: the desired target (label or value) â€” only available in supervised learning\n",
    "\n",
    "- Inputs are typically organized into a **Feature Matrix** `X`:\n",
    "  $$\n",
    "  X = \\begin{bmatrix}\n",
    "  x_{11} & x_{12} & \\dots & x_{1D} \\\\\n",
    "  x_{21} & x_{22} & \\dots & x_{2D} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_{N1} & x_{N2} & \\dots & x_{ND}\n",
    "  \\end{bmatrix}\n",
    "  \\quad \\Rightarrow \\quad\n",
    "  X = [x_1, x_2, \\dots, x_N]^T \\in \\mathbb{R}^{N \\times D}\n",
    "  $$\n",
    "\n",
    "  - `N` = number of samples\n",
    "  - `D` = number of features (dimensions)\n",
    "\n",
    "### What is a Feature?\n",
    "\n",
    "A **feature** is a measurable property or attribute relevant to the problem.\n",
    "\n",
    "**Classic Example: Iris Dataset** (Classification of 3 iris flower species)\n",
    "\n",
    "![Iris Features](images/features-iris.png)\n",
    "\n",
    "- Features: Sepal Length, Sepal Width, Petal Length, Petal Width\n",
    "- Dataset size: 150 samples â†’ $X \\in \\mathbb{R}^{150 \\times 4}$\n",
    "\n",
    "## Training and Test Datasets\n",
    "\n",
    "![Training and Test Split](images/train-test-split.png)\n",
    "\n",
    "![Data Distribution](images/data-distribution.png)\n",
    "\n",
    "- We always split data into **at least two distinct sets**:\n",
    "  - **Training Dataset**: Used to learn the function `f(x)`\n",
    "  - **Test Dataset**: Used to evaluate performance after training\n",
    "\n",
    "**Important Rules:**\n",
    "\n",
    "- Training and test examples **must be different**\n",
    "- Why? If the model sees test data during training, we **overestimate** its real-world performance (like giving a student the exam answers as homework!)\n",
    "\n",
    "**Assumptions for Good Generalization:**\n",
    "\n",
    "- Training and test samples are drawn from the **same underlying data distribution** $P_{data}$\n",
    "- Samples are **independent and identically distributed (i.i.d.)**\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "![Supervised Learning](images/supervised-learning.png)\n",
    "\n",
    "Supervised learning = learning from **labeled** examples (both `x` and correct `y` are provided).\n",
    "\n",
    "- Covers both **classification** and **regression** problems\n",
    "\n",
    "In matrix form:\n",
    "- Feature matrix: $X \\in \\mathbb{R}^{N \\times D}$\n",
    "- Target matrix: $Y \\in \\mathbb{R}^{N \\times K}$ (K = number of outputs/classes)\n",
    "\n",
    "**Common Supervised Algorithms:**\n",
    "\n",
    "- Linear models\n",
    "- Neural networks\n",
    "- Support vector machines\n",
    "- Naive Bayes\n",
    "- K-nearest neighbors\n",
    "- Random forests\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised learning = learning from **unlabeled** data (only inputs `x` are available).\n",
    "\n",
    "> Even without labels, we can discover useful structure in the data.\n",
    "\n",
    "### Common Tasks\n",
    "\n",
    "1. **Clustering**  \n",
    "   Group similar data points together into clusters.\n",
    "\n",
    "   ![Clustering Example](images/clustering-example.png)\n",
    "\n",
    "   - We detect \"close\" points and assign them to the same cluster.\n",
    "   - No predefined labels â€” clusters represent different types of inputs.\n",
    "\n",
    "2. **Probability Density Estimation**  \n",
    "   Estimate the underlying data distribution $P_{data}$.\n",
    "\n",
    "    ![Density Estimation Example](images/density-estimation.png)  \n",
    "   - Learn how likely different points are.\n",
    "   - Useful for anomaly detection, generating new samples, etc.\n",
    "\n",
    "**Feature Matrix (same as supervised):**  \n",
    "$X \\in \\mathbb{R}^{N \\times D}$ (no `Y` matrix)\n",
    "\n",
    "**Applications:**\n",
    "- Customer segmentation\n",
    "- Anomaly detection\n",
    "- Data compression (dimensionality reduction)\n",
    "- Preprocessing for supervised tasks\n",
    "\n",
    "## Reinforcement Learning (Brief Overview)\n",
    "\n",
    "![Reinforcement Learning](images/reinforcement-learning.png)\n",
    "\n",
    "- No fixed labeled dataset\n",
    "- Learning by **interaction** with an environment\n",
    "- Agent takes **actions** â†’ receives **rewards/penalties**\n",
    "- Goal: Maximize cumulative reward\n",
    "\n",
    "**Applications:**\n",
    "- Game playing (AlphaGo)\n",
    "- Robotics\n",
    "- Self-driving cars\n",
    "- Drug discovery\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebc37f",
   "metadata": {},
   "source": [
    "# Basic Components of a Machine Learning Model\n",
    "\n",
    "A machine learning system has a few fundamental building blocks. Understanding these helps demystify how models work.\n",
    "\n",
    "## Machine Learning Algorithm\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "A machine learning **algorithm** is a procedure that finds a function $\\hat{f}$ mapping inputs to outputs:\n",
    "\n",
    "$$\\hat{y} = \\hat{f}(x)$$\n",
    "\n",
    "Where:\n",
    "- $x \\in \\mathbb{R}^D$ â†’ Input feature vector (D dimensions)\n",
    "- $\\hat{y} \\in \\mathbb{R}^K$ â†’ Predicted output (K dimensions, e.g., class probabilities or values)\n",
    "\n",
    "The algorithm does **not** search all possible functions in the universe. Instead, it searches within a restricted set called the **hypothesis space** $\\mathcal{H}$.\n",
    "\n",
    "### Hypothesis Space $\\mathcal{H}$\n",
    "\n",
    "- $\\mathcal{H}$ = the set of all functions (models) that the algorithm is capable of selecting.\n",
    "- Each function $f_i \\in \\mathcal{H}$ is a **possible solution** (a candidate model).\n",
    "\n",
    "    ![Hypothesis Space Illustration](images/hypothesis-space.png)\n",
    "\n",
    "Examples of hypothesis spaces:\n",
    "- Linear models: $\\mathcal{H}$ = all straight lines (or hyperplanes)\n",
    "- Decision trees: $\\mathcal{H}$ = all possible tree structures\n",
    "- Neural networks: $\\mathcal{H}$ = all networks with a given architecture\n",
    "\n",
    "### Objective (Loss) Function $J$\n",
    "\n",
    "For every candidate function $f_i$, we evaluate how well it performs on the training data using an **objective function**:\n",
    "\n",
    "$$J(Y, f_i(X))$$\n",
    "\n",
    "- Measures the error or \"cost\" of predictions\n",
    "- Lower $J$ = better fit to training data\n",
    "\n",
    "Common examples:\n",
    "- Mean Squared Error (regression)\n",
    "- Cross-Entropy Loss (classification)\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The algorithm explores the hypothesis space $\\mathcal{H}$ to find the function $\\hat{f}$ that **minimizes** the objective:\n",
    "\n",
    "$$\\hat{f} = \\arg\\min_{f \\in \\mathcal{H}} J(Y, f(X))$$\n",
    "\n",
    "In practice:\n",
    "- We iteratively test different functions (or adjust parameters)\n",
    "- Move toward functions with lower training error\n",
    "- Stop when we find a function that \"explains the training data well\"\n",
    "\n",
    "## Summary of Key Components\n",
    "\n",
    "| Component              | Symbol/Name             | Role                                                                 |\n",
    "|------------------------|-------------------------|----------------------------------------------------------------------|\n",
    "| **Input**              | $x$                     | Features of a data point                                             |\n",
    "| **Output/Prediction**  | $\\hat{y}$               | Model's predicted target                                             |\n",
    "| **Model/Function**     | $\\hat{f}$ or $f \\in \\mathcal{H}$ | Mapping learned by the algorithm                                     |\n",
    "| **Hypothesis Space**   | $\\mathcal{H}$           | All possible models the algorithm can choose from                    |\n",
    "| **Objective Function** | $J$                     | Measures how good a candidate model is on training data              |\n",
    "| **Training Data**      | $(X, Y)$                | Used to evaluate $J$ and guide the search in $\\mathcal{H}$            |\n",
    "\n",
    "These components are present in **every** machine learning algorithm, regardless of whether it's linear regression, a neural network, or clustering.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c3370",
   "metadata": {},
   "source": [
    "# Objective (Loss) Function\n",
    "\n",
    "The **objective function** (also called **loss function**, **cost function**, or **criterion**) quantifies how well a candidate model fits the training data.\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- Training aims to find a function $\\hat{f}$ that \"fits well\" the training data.\n",
    "- To measure \"how good\" the predictions are, we define an **objective function** $J$:\n",
    "\n",
    "$$J(Y, f(X)) : \\mathbb{R}^{N \\times K} \\to \\mathbb{R}$$\n",
    "\n",
    "- By convention, we usually **minimize** $J$ (lower = better).\n",
    "- If we minimize it, it is often called **loss** or **error**.\n",
    "\n",
    "### Empirical Risk Minimization\n",
    "\n",
    "Most ML algorithms compute the objective as an average loss over training samples:\n",
    "\n",
    "$$J(Y, f(X)) = \\frac{1}{N} \\sum_{i=1}^{N} L(f(x_i), y_i)$$\n",
    "\n",
    "- $L(f(x_i), y_i)$ = loss for a single example (also called **individual loss** or **criterion**)\n",
    "- The overall training goal is **empirical risk minimization** â€” minimize the average loss on training data.\n",
    "\n",
    "## Common Loss Functions\n",
    "\n",
    "### For Regression: Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "\n",
    "A popular choice for regression:\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - f(x_i))^2$$\n",
    "\n",
    "- Measures squared distance between predictions and targets\n",
    "- Intuitively penalizes larger errors more\n",
    "- Lower MSE = better fit\n",
    "\n",
    "### For Classification\n",
    "\n",
    "#### Accuracy (Hard Metric)\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "$$Acc = \\frac{N_{correct}}{N_{tot}}$$\n",
    "\n",
    "- Easy to interpret\n",
    "- Common evaluation metric (especially on test set)\n",
    "- But it's a **hard** metric â€” doesn't reflect confidence\n",
    "- Two models can have same accuracy but very different confidence levels\n",
    "\n",
    "#### Categorical Cross-Entropy (Soft Metric)\n",
    "\n",
    "\n",
    "A \"softer\" and preferred training loss:\n",
    "\n",
    "$$CCE = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\ln(\\hat{p}_{ik})$$\n",
    "\n",
    "- Uses **one-hot** labels and **softmax probabilities** $\\hat{p}$\n",
    "- Penalizes confident wrong predictions heavily\n",
    "- Lower cross-entropy = better (0 = perfect, higher = worse)\n",
    "- Ranges from 0 (perfect) to +âˆž (very bad)\n",
    "\n",
    "**Key Advantage:**  \n",
    "Models with same accuracy but higher confidence get lower (better) cross-entropy.\n",
    "\n",
    "## Summary of Common Objectives\n",
    "\n",
    "| Task           | Common Objective/Loss                  | Notes                                      |\n",
    "|----------------|----------------------------------------|--------------------------------------------|\n",
    "| **Regression** | Mean Squared Error (MSE)               | Sensitive to outliers, easy to optimize    |\n",
    "| **Classification** | Categorical Cross-Entropy (CCE)    | Standard for multi-class, uses probabilities |\n",
    "| **Classification (evaluation)** | Accuracy                      | Simple, interpretable, but \"hard\" metric   |\n",
    "\n",
    "The choice of objective function is crucial â€” it directly guides what the algorithm learns!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11068b6",
   "metadata": {},
   "source": [
    "# Optimization in Machine Learning\n",
    "\n",
    "Optimization is the process of finding the best model (function) from the hypothesis space that minimizes the objective function on the training data.\n",
    "\n",
    "\n",
    "## The Optimization Problem\n",
    "\n",
    "We want to solve:\n",
    "\n",
    "$$f^* = \\arg\\min_{f \\in \\mathcal{H}} J(Y, f(X))$$\n",
    "\n",
    "- Search within the hypothesis space $\\mathcal{H}$\n",
    "- Find the function with the **lowest** objective value $J$\n",
    "- This procedure is performed by an **optimizer**\n",
    "\n",
    "The loss landscape can be complex (high-dimensional, non-convex), so exact solutions are often impossible â†’ we use iterative algorithms.\n",
    "\n",
    "## Naive Optimization Example: Curve Fitting\n",
    "\n",
    "Let's see a simple (naive) example to understand the idea.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- Training set: scattered points $(x_i, y_i)$\n",
    "- Goal: Find $f(x): \\mathbb{R} \\to \\mathbb{R}$ that fits the data well\n",
    "\n",
    "### Hypothesis Space (Toy Example)\n",
    "\n",
    "**Naive approach**:   try all the functions of the hypothesis space and take the one that better explains the training data.\n",
    "\n",
    "We limit ourselves to only 4 candidate functions:\n",
    "\n",
    "1. $f(x) = x$\n",
    "2. $f(x) = e^x$\n",
    "3. $f(x) = \\sin(x)$\n",
    "4. $f(x) = \\cos(x)$\n",
    "\n",
    "### Naive Approach\n",
    "\n",
    "- Evaluate **Mean Squared Error (MSE)** for each candidate on the training data\n",
    "- Pick the one with the lowest MSE\n",
    "\n",
    "Results:\n",
    "- $f(x) = \\sin(x)$ has the lowest training MSE (~0.013) â†’ **Winner!**\n",
    "\n",
    "### Check Generalization\n",
    "\n",
    "- Apply the winning function to a **test set** (new points)\n",
    "- Compute test MSE (~0.010) â†’ very low\n",
    "- Conclusion: The learned function generalizes well ðŸ˜Š\n",
    "\n",
    "**Note:** This naive \"try all\" approach only works for tiny hypothesis spaces. Real ML has billions/trillions of possible functions â†’ we need smarter optimizers.\n",
    "\n",
    "## Real-World Optimization\n",
    "\n",
    "In practice, we use **iterative optimization algorithms** like:\n",
    "\n",
    "- **Gradient Descent** (and variants: SGD, Adam, RMSProp)\n",
    "- These efficiently navigate the high-dimensional loss landscape\n",
    "- Start from a random initialization\n",
    "- Repeatedly update model parameters to reduce $J$\n",
    "\n",
    "Weâ€™ll cover Gradient Descent in detail next!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959d8f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
